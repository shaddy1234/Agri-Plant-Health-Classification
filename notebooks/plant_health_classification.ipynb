{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a207eb",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "\n",
    "## Project Title: Agricultural Plant Health Classification\n",
    "\n",
    "### Context\n",
    "Early detection of plant diseases is crucial for maintaining crop health and maximizing yield. Manually inspecting plants is time-consuming and error-prone. This project focuses on developing a classification system to automatically distinguish between healthy and unhealthy plants based on their photographs.\n",
    "\n",
    "### Objective\n",
    "Develop a machine learning model to automatically classify plant images into:\n",
    "- **Healthy Plants**: No disease or stress\n",
    "- **Unhealthy Plants**: Showing signs of disease, pest damage, or stress\n",
    "\n",
    "### Expected Outcome\n",
    "A working proof-of-concept system that predicts the health status of a plant based on images.\n",
    "\n",
    "### Application\n",
    "Farmers and agricultural experts can use this system for timely intervention to prevent crop damage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c9b71-249c-4757-83e9-12541085a72b",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "### Source\n",
    "- Used publicly available dataset from Mendeley Data [Bangladesh Dataset](https://data.mendeley.com/datasets/3wby28tkcp/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d47bc7-a0e6-4037-a575-e4bc9b5cfb08",
   "metadata": {},
   "source": [
    "## Dataset Selection and Preparation\n",
    "\n",
    "### Focus Crop: Bean\n",
    "\n",
    "The dataset used in this project is the **Vegetables Dataset**, which contains images of four different vegetable crops, each divided into healthy and unhealthy categories:\n",
    "```\n",
    "Vegetables Dataset/\n",
    "├── Malabar/\n",
    "│ ├── Healthy/\n",
    "│ └── Unhealthy/\n",
    "├── Brinjal/\n",
    "│ ├── Healthy/\n",
    "│ └── Unhealthy/\n",
    "├── Cauliflower/\n",
    "│ ├── Healthy/\n",
    "│ └── Unhealthy/\n",
    "└── Bean/\n",
    "├── Healthy/\n",
    "└── Unhealthy/\n",
    "```\n",
    "\n",
    "For this **proof-of-concept project**, we focus only on the **Bean** crop, performing a **binary classification**:\n",
    "\n",
    "- **Healthy Beans → label 0**  \n",
    "- **Unhealthy Beans → label 1**\n",
    "\n",
    "### Reasoning\n",
    "\n",
    "Using only a single crop type has several advantages:\n",
    "\n",
    "1. **Reduces variability**: Different crops have different leaf shapes, sizes, and colors. Combining all crops into one model can confuse the classifier and reduce predictive accuracy.\n",
    "2. **Simplifies the baseline model**: By focusing on Bean, the model learns patterns specific to this crop, allowing us to establish a strong baseline before expanding to multiple crops.\n",
    "3. **Demonstrates ML workflow**: The goal is to show end-to-end model building — loading data, preprocessing, feature extraction, training, and evaluation — on a manageable and interpretable subset.\n",
    "\n",
    "### Folder Structure for Bean\n",
    "\n",
    "The **Bean crop images** are organized as follows:\n",
    "```\n",
    "data/\n",
    "└── Bean/\n",
    "├── Healthy/ # Images of healthy bean plants\n",
    "└── Unhealthy/ # Images of diseased or stressed bean plants\n",
    "```\n",
    "\n",
    "Each subfolder corresponds to a **class label**, which will be used during model training.\n",
    "\n",
    " \n",
    "\n",
    "By focusing on this single crop type, the model can learn clear distinguishing patterns between healthy and unhealthy plants, making this a clean **baseline for future expansion** to multi-class or multi-crop classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8b682-751a-4734-8700-0aae3b68ba21",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In this section, we prepare the Bean plant images for machine learning. Since we are performing **binary classification** (Healthy vs. Unhealthy), we need to convert the raw images into a format suitable for a **classical ML model** like Logistic Regression.  \n",
    "\n",
    "Steps taken:\n",
    "\n",
    "1. **Image Resizing**  \n",
    "   - All images are resized to a uniform size (64x64 pixels) to ensure consistency across the dataset.  \n",
    "   - Resizing helps reduce computational load while retaining enough detail for the model to learn.\n",
    "\n",
    "2. **Color Conversion and Flattening**  \n",
    "   - Images are converted to RGB to ensure all have 3 color channels.  \n",
    "   - Each image is flattened into a 1-dimensional array of pixel values so that classical ML models can process them.  \n",
    "   - Flattening turns a 64x64x3 image into a 12,288-length feature vector.\n",
    "\n",
    "3. **Label Encoding**  \n",
    "   - Folder names (`Healthy` and `Unhealthy`) are mapped to numeric labels:  \n",
    "     - Healthy - 0  \n",
    "     - Unhealthy - 1  \n",
    "\n",
    "4. **Feature and Label Arrays**  \n",
    "   - All processed images are stored in `X` (features).  \n",
    "   - Corresponding labels are stored in `y` (target).  \n",
    "\n",
    "5. **Verification**  \n",
    "   - We check the shape of `X` and `y` to ensure the data is loaded correctly:  \n",
    "     - `X.shape` - `(number_of_images, 12288)`  \n",
    "     - `y.shape` - `(number_of_images,)`  \n",
    "\n",
    "This preprocessing ensures the images are ready for training a **binary classification model** while keeping the workflow simple and interpretable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "121640c0-96b1-4b9d-a38d-725ee857ddee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Image Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Healthy: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 632/632 [01:43<00:00,  6.09it/s]\n",
      "Loading Unhealthy: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verification ---\n",
      "Features shape (X): (645, 12288)\n",
      "Labels shape (y): (645,)\n",
      "Sample Label (first image): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm # adds a nice loading bar\n",
    "\n",
    "# 1. Define Paths\n",
    "BASE_PATH = '/home/shaddy/Downloads/Dataset on Bangladeshi Healthy and Unhealthy Veget/Vegetables/Bean'\n",
    "CATEGORIES = ['Healthy', 'Unhealthy']\n",
    "IMG_SIZE = 64\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# 2. The Wrangling Loop\n",
    "print(\"Starting Image Processing...\")\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(BASE_PATH, category)\n",
    "    label = CATEGORIES.index(category) # Healthy=0, Unhealthy=1\n",
    "    \n",
    "    for img_name in tqdm(os.listdir(path), desc=f\"Loading {category}\"):\n",
    "        try:\n",
    "            # Read and Convert\n",
    "            img_path = os.path.join(path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Step 1: Resize\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "            # Step 2: Flattening (64*64*3 = 12288)\n",
    "            flattened_img = img.flatten()\n",
    "            \n",
    "            X.append(flattened_img)\n",
    "            y.append(label)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "# Step 4: Convert to Arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Step 6: Normalization\n",
    "X = X / 255.0\n",
    "\n",
    "# Step 5: Verification\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"Features shape (X): {X.shape}\") # Expect (num_images, 12288)\n",
    "print(f\"Labels shape (y): {y.shape}\")     # Expect (num_images,)\n",
    "print(f\"Sample Label (first image): {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18615a9-1589-4c91-b5b8-c893de26624f",
   "metadata": {},
   "source": [
    "**Note on Class Imbalance**\n",
    "\n",
    "After loading the images, we notice that the dataset is highly imbalanced:\n",
    "\n",
    "- Healthy: 632 images\n",
    "- Unhealthy: 13 images\n",
    "\n",
    "This imbalance can affect model performance, especially for minority classes.  \n",
    "In a production scenario, techniques like **class weighting, oversampling the minority class and others** could help mitigate this issue.  \n",
    "\n",
    "For this proof-of-concept, we proceed with the dataset as-is, but the imbalance is noted for consideration during evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bba458-a2c2-4264-bbba-d08a66314bcf",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "We train a Logistic Regression model to classify Bean plant images as Healthy (0) or Unhealthy (1).\n",
    "\n",
    "- The training and test sets are split with an 80/20 ratio, preserving class distribution (`stratify=y`).\n",
    "- Logistic Regression is used with `class_weight='balanced'` to partially account for the imbalanced dataset.\n",
    "- Evaluation to be  done using Accuracy, Confusion Matrix, and classification metrics (Precision, Recall, F1-score) to understand model performance on both classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b89128-ec5f-40b6-b3ee-7f6bf5785706",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "We split our data into training and testing sets so you can evaluate performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a4af28f-a43b-4f0b-83f2-95e31927b2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (516, 12288) (516,)\n",
      "Test set: (129, 12288) (129,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b2298-4f64-4cf4-8420-b7e4c75fb762",
   "metadata": {},
   "source": [
    "### Logistic Regression Implementation\n",
    "- I implemented LR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e904ed61-458b-4aac-be0f-fcacef642d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9457364341085271\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       126\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.95       129\n",
      "   macro avg       0.49      0.48      0.49       129\n",
      "weighted avg       0.95      0.95      0.95       129\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[122   4]\n",
      " [  3   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, class_weight='balanced')  # 'balanced' helps with class imbalance\n",
    "\n",
    "# Train\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee4d24-8392-4a96-89ec-feb17507a205",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "The Logistic Regression model achieved 94.5% accuracy. However, due to the severe class imbalance (126 Healthy vs 3 Unhealthy in the test set), the model fails to correctly classify any Unhealthy plants. \n",
    "\n",
    "- All Unhealthy plants in the test set were misclassified as Healthy.\n",
    "- This highlights the importance of handling class imbalance, e.g., through oversampling, class weighting, or collecting more Unhealthy images.\n",
    "- While overall accuracy is high, the Recall for the 'Unhealthy' class is 0%. In an agricultural context, this is a 'False Negative' and is the most costly error. This proves that the model is simply 'learning' the distribution of the data (predicting the majority class) rather than learning the features of the disease.\"\n",
    "\n",
    "For this proof-of-concept, I document the results and noted that the model performs well for the majority class but poorly for the minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b7785-1a65-4b58-b9a4-5e8041ebbb81",
   "metadata": {},
   "source": [
    "# Conclusion, Reflection, and Challenges\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project demonstrates an end-to-end workflow for binary image classification of Bean plant health using classical machine learning. The key steps and observations are summarized below:\n",
    "\n",
    "1. **Problem Definition**\n",
    "   - Objective: Automatically classify Bean plant images as Healthy (0) or Unhealthy (1) to support early detection of plant stress or disease.\n",
    "   - Dataset: Bean images from the Vegetables Dataset, divided into Healthy and Unhealthy categories.\n",
    "\n",
    "2. **Data Loading**\n",
    "   - Images were loaded from folder structure (`Healthy` and `Unhealthy`) using OpenCV.\n",
    "   - Ensured all images were RGB and resized to 64x64 pixels for consistency.\n",
    "   - Flattened images into 1D arrays suitable for classical ML models.\n",
    "\n",
    "3. **Data Preprocessing**\n",
    "   - Converted images into feature vectors (`X`) and labels (`y`).\n",
    "   - Normalized pixel values to range [0,1].\n",
    "   - Verified shapes: `X.shape = (645, 12288)`, `y.shape = (645,)`.\n",
    "   - Noted class imbalance (632 Healthy vs 13 Unhealthy), which could impact model performance.\n",
    "\n",
    "4. **Modeling**\n",
    "   - Applied Logistic Regression, a classical ML model aligned with claimed skills.\n",
    "   - Split data into training and test sets (80/20), with stratification to maintain class distribution.\n",
    "   - Used `class_weight='balanced'` to partially account for class imbalance.\n",
    "\n",
    "5. **Evaluation**\n",
    "   - Overall accuracy: **94.5%**\n",
    "   - Confusion matrix and classification report reveal that all Unhealthy plants were misclassified, highlighting the effect of class imbalance.\n",
    "   - Demonstrates the importance of evaluating metrics beyond accuracy, especially with imbalanced datasets.\n",
    "\n",
    "\n",
    "\n",
    "## Reflection and Future Directions\n",
    "\n",
    "After completing this proof-of-concept, I have noted that using **Computer Vision techniques and deep learning models (e.g., Convolutional Neural Networks)** could improve performance significantly. Unlike classical ML models, deep learning:\n",
    "\n",
    "- Automatically learns complex features from images such as shapes, textures, and patterns.  \n",
    "- Reduces the need for manual feature engineering or flattening of images.  \n",
    "- Can better handle variability in leaf shapes, lighting conditions, and background noise.  \n",
    "- May improve classification of minority classes (e.g., Unhealthy plants) if combined with data augmentation techniques.\n",
    "\n",
    "If I were to extend this project, I could:\n",
    "\n",
    "- Collect more Unhealthy plant images to balance the dataset.  \n",
    "- Experiment with CNN architectures to directly process images without flattening.  \n",
    "- Apply data augmentation to create a richer training set and improve model generalization.  \n",
    "- Compare classical ML performance with deep learning to evaluate trade-offs in simplicity vs accuracy.\n",
    "\n",
    "\n",
    "\n",
    "## Challenges Encountered\n",
    "\n",
    "During this project, I faced several challenges, which reflect both technical and learning aspects:\n",
    "\n",
    "1. **Data Collection and Preparation**\n",
    "   - The dataset was downloaded from Mendeley, but I had to locate the specific Bean crop images and ensure folder structure matched labels.\n",
    "   - Managing imbalanced classes (632 Healthy vs 13 Unhealthy) posed challenges in model evaluation.\n",
    "\n",
    "2. **Learning and Applying Classification**\n",
    "   - My prior experience was mainly with regression; I had to quickly learn the basics of binary classification, evaluation metrics (precision, recall, F1-score), and how to interpret confusion matrices.\n",
    "   - Figuring out how to convert images into numerical features suitable for Logistic Regression (flattening, normalization) required research and experimentation.\n",
    "\n",
    "3. **Time Constraints**\n",
    "   - The project had to be completed in 3 days, which meant I had to learn, implement, and test quickly while ensuring a coherent, reproducible workflow.\n",
    "\n",
    "4. **Balancing Simplicity vs Performance**\n",
    "   - Choosing Logistic Regression was a decision to stay within my known skillset, even though after research I found out deep learning could perform better.\n",
    "   - Highlighting class imbalance and thinking about future improvements shows awareness of model limitations without overcomplicating the current implementation.\n",
    "\n",
    "5. **Technical Hurdles**\n",
    "   - Reading images consistently  \n",
    "   - Resizing and flattening images for classical ML  \n",
    "   - Not a hurdel>> Data without errors was a boost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f04de7-c6b8-4e46-9660-9b804d5d65ec",
   "metadata": {},
   "source": [
    "## Production-Oriented Approach \n",
    "\n",
    "In a production setting where computer vision is a core requirement, this system would be extended using a CNN-based architecture. A pretrained model (e.g. ResNet or MobileNet) could be fine-tuned on the Bean dataset to automatically learn spatial and texture-based features.\n",
    "\n",
    "This approach would:\n",
    "- Remove the need for manual feature flattening\n",
    "- Improve robustness to lighting and background variation\n",
    "- Improve minority class detection through data augmentation\n",
    "\n",
    "Due to time constraints and my current learning stage, this project focuses on a classical ML baseline while clearly outlining the path to a production-ready CV system.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
